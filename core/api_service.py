import os
import logging
import threading
from google import genai
from google.genai import types
from dotenv import load_dotenv

load_dotenv()

class APIService:
    def __init__(self):
        self.api_key = os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            logging.error("âŒ API Key missing!")
            return

        self.client = genai.Client(
            api_key=self.api_key,
            http_options={'api_version': 'v1beta'}
        )

        self.model_name = "gemini-2.0-flash" # Veya gemini-1.5-flash
        
        # LATENCY OPTÄ°MÄ°ZASYONU 1: System Prompt'u Token Tasarrufu Ä°Ã§in KÄ±salttÄ±k
        # Eski: "Sen akademik bir Ã§evirmensin..." (~20 token)
        # Yeni: "TR Ã‡eviri. Akademik. Sadece metin." (~6 token) -> Etki aynÄ±, hÄ±z daha yÃ¼ksek.
        self.stream_config = types.GenerateContentConfig(
            temperature=0.3,
            max_output_tokens=8192,
            system_instruction="Translate to Turkish. Academic style. Output only translation."
        )

    def warmup(self):
        """
        LATENCY OPTÄ°MÄ°ZASYONU 2: Connection Warm-up
        Ä°lk baÄŸlantÄ± maliyetini (SSL Handshake) uygulama aÃ§Ä±lÄ±ÅŸÄ±nda Ã¶der.
        KullanÄ±cÄ± ilk Ã§evirisini yaparken hat hazÄ±r olur.
        """
        def _warmup_task():
            try:
                logging.info("ğŸ”¥ API IsÄ±nma turu baÅŸladÄ±...")
                # Tek tokenlÄ±k boÅŸ bir istek
                self.client.models.generate_content(
                    model=self.model_name,
                    contents="Hi",
                    config=types.GenerateContentConfig(max_output_tokens=1)
                )
                logging.info("âœ… API IsÄ±ndÄ± ve hazÄ±r!")
            except Exception as e:
                logging.warning(f"IsÄ±nma hatasÄ± (Ã–nemli deÄŸil): {e}")

        # Ana akÄ±ÅŸÄ± bloklamamak iÃ§in thread iÃ§inde Ã§alÄ±ÅŸtÄ±r
        threading.Thread(target=_warmup_task, daemon=True).start()

    def translate_text_stream(self, text):
        if not text: return
        try:
            response = self.client.models.generate_content_stream(
                model=self.model_name,
                contents=text,
                config=self.stream_config
            )
            for chunk in response:
                if chunk.text:
                    yield chunk.text
        except Exception as e:
            logging.error(f"âŒ API Stream Error: {e}")
            yield f" [Hata: {str(e)}]"
